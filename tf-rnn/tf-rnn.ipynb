{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "chars.insert(0, \"\\0\")\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_indices = {c:i for i,c in enumerate(chars)}\n",
    "indices_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+n] for i in range(0, len(idx)-1-sequence_len, sequence_len)]\n",
    "            for n in range(sequence_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [[idx[i+n] for i in range(1, len(idx)-sequence_len, sequence_len)]\n",
    "            for n in range(sequence_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = [np.stack(c[:-2]) for c in c_in_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = [np.stack(c[:-2]) for c in c_out_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75109, 8, 85), (75109, 8, 85))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_ys = [to_categorical(o, vocab_size) for o in ys]\n",
    "oh_y_rnn=np.stack(oh_ys, axis=1)\n",
    "\n",
    "oh_xs = [to_categorical(o, vocab_size) for o in xs]\n",
    "oh_x_rnn=np.stack(oh_xs, axis=1)\n",
    "\n",
    "oh_x_rnn.shape, oh_y_rnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're defining the matrices to act on a single character at a time, then we'll use the TF scan (which is kind of like a for loop but easily parallelizable) to compute across a whole sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_in = vocab_size #aka 85\n",
    "n_hidden = 256\n",
    "n_out = vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here we use the Glorot initialisation for the two random weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_h = tf.Variable(tf.eye(n_hidden))\n",
    "B_h = tf.Variable(tf.zeros([1, n_hidden]))\n",
    "\n",
    "W_x = tf.Variable(tf.random_normal([n_in, n_hidden], stddev=math.sqrt(2/n_in)))\n",
    "B_x = tf.Variable(tf.zeros([1, n_hidden]))\n",
    "\n",
    "W = tf.concat([W_h, W_x], 0)\n",
    "B = B_h+B_x\n",
    "\n",
    "W_y = tf.Variable(tf.random_normal([n_hidden, n_out], stddev=math.sqrt(2/n_hidden)))\n",
    "B_y = tf.Variable(tf.zeros([sequence_len, 1, n_out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Model placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = tf.placeholder(tf.float32, [1, n_hidden])\n",
    "t_inp = tf.placeholder(tf.float32, [sequence_len, n_in])\n",
    "t_out = tf.placeholder(tf.float32, [sequence_len, n_out])\n",
    "lr = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step(h,x):\n",
    "    #h, _ = h\n",
    "    #Can concatenate the tensors to turn this into a single matrix multiplication\n",
    "    #Also expand dims of the input vector so the concat works (which is higher \n",
    "    #rank so that TF can treat it as a matrix in .matmul)\n",
    "    a = tf.concat([h,tf.expand_dims(x,0)], 1)\n",
    "    h = tf.nn.relu(tf.matmul(a,W) + B)\n",
    "    output = tf.nn.softmax(tf.matmul(h, W_y) + B_y)\n",
    "    #return h, output\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_out = tf.scan(step, t_inp, initializer=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'scan/TensorArrayStack/TensorArrayGatherV3:0' shape=(8, 1, 256) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze:0' shape=(8, 85) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.tensordot(rnn_out, W_y, [[2],[0]]) + B_y\n",
    "y = tf.squeeze(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=t_out, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 3.01136\n",
      "train loss 2.78089\n",
      "train loss 2.71054\n",
      "train loss 2.87373\n",
      "train loss 2.73607\n",
      "train loss 2.78415\n",
      "train loss 2.53392\n",
      "train loss 2.46605\n",
      "train loss 2.51161\n",
      "train loss 2.30324\n",
      "lowering learning rate to 0.01\n",
      "train loss 2.57934\n",
      "train loss 2.58376\n",
      "train loss 2.70472\n",
      "train loss 2.596\n",
      "train loss 2.6155\n",
      "train loss 2.60336\n",
      "train loss 2.62175\n",
      "train loss 2.56545\n",
      "train loss 2.62515\n",
      "train loss 2.61964\n",
      "lowering learning rate to 0.001\n",
      "train loss 2.86738\n",
      "train loss 2.68616\n",
      "train loss 2.64321\n",
      "train loss 2.63148\n",
      "train loss 2.57552\n",
      "train loss 2.68501\n",
      "train loss 2.63017\n",
      "train loss 2.81565\n",
      "train loss 2.58217\n",
      "train loss 2.85336\n",
      "lowering learning rate to 0.0001\n",
      "train loss 2.71129\n",
      "train loss 2.59715\n",
      "train loss 2.62834\n",
      "train loss 2.58449\n",
      "train loss 2.78552\n",
      "train loss 2.71551\n",
      "train loss 2.65559\n",
      "train loss 2.55966\n",
      "train loss 2.71746\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    learning = 0.1\n",
    "    err = 0.0\n",
    "    for i in range(len(oh_x_rnn)):\n",
    "        feed_dict = {\n",
    "            h:np.zeros([1,n_hidden]),\n",
    "            t_inp:oh_x_rnn[i],\n",
    "            t_out:oh_y_rnn[i],\n",
    "            lr:learning\n",
    "        }\n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "        err += cross_entropy.eval(feed_dict)\n",
    "        if i % 100 == 99:\n",
    "            print('train loss %g' % (err/100))\n",
    "            err = 0.0\n",
    "        if i % 1000 == 999:\n",
    "            print('lowering learning rate to %g' % (learning/10))\n",
    "            learning = learning/10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-rnn]",
   "language": "python",
   "name": "conda-env-tf-rnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
