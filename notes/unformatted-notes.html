<!doctype html><html><head><meta charset="utf-8"><style>
      /* Separated from document_view.css to enable use in downloadable
   export files. #cutAndCopyBucket is included for every rule because
   we wwant cut/copied items to be formatted the same way as they
   would be when exported. */

.formattedExport,
#cutAndCopyBucket {
    font-family: 'Helvetica Neue', Arial, Sans-serif;
    color: #333;
    font-size: 13px;
    line-height: 17px;
    /* used to set white-space: pre-wrap here, but that messes up GMail's
       formatting of the export output, so moving that to the name and
       notes separately. */
}

.formattedExport .name, .formattedExport .note,
#cutAndCopyBucket .name, #cutAndCopyBucket .note {
    white-space: pre-wrap;
}

.formattedExport ul,
#cutAndCopyBucket ul{
    list-style: disc;
    /* Needed to reset browser defaults. */
    margin: 0;
    padding: 0;
}

.formattedExport li,
#cutAndCopyBucket li {
    margin: 4px 0 4px 20px;
    /* Needed to reset browser defaults. */
    padding: 0;
}

.formattedExport > .name,
#cutAndCopyBucket > .name {
    font-size: 16px;
    line-height: 21px;
}

.formattedExport > .note,
#cutAndCopyBucket > .note {
    font-size: 13px;
    line-height: 17px;
}

.formattedExport > ul,
#cutAndCopyBucket > ul {
    margin-top: 15px;
}

.formattedExport .name.done,
#cutAndCopyBucket .name.done {
    text-decoration:line-through;
    color:#999;
}

.formattedExport .note,
#cutAndCopyBucket .note {
    font-size: 12px;
    color:#666;
}

      </style></head><body class="formattedExport"><ul><li><span class="name">Fast.ai PART 1 course</span><ul><li><span class="name">Lecture1 Finetuning catsndogs #lecture1</span><ul><li><span class="name">#setup Directory structure v important - why not IMMEDIATELY set up a structure of</span><ul><li><span class="name">data</span><ul><li><span class="name">train, validate, test1,..., sample</span></li><li><span class="name">in each of train/validate have a folder for each label aka cats,dogs</span></li><li><span class="name">in test1 have unknown/ then all the images</span></li><li><span class="name">in sample have train,validate again but with very few images just copied (for fast prototyping that things are running properly)</span></li><li><span class="name">then each image have its ID as its filename</span></li></ul></li></ul></li><li><span class="name">#setup Then parsing this is really easy - EG keras.preprocessing.image.ImageDataGenerator() has an amazing method flow_from_directory which batches this up for you! then give it to fit_generator (it handles one-hotting and everything, and easily does test data too. so good)</span></li><li><span class="name">#todo set up equiv of their Vgg16 class that allows for very quick finetuning but for ANY pretrained net - just replace that last layer with a dense layer of the right size (take ques from their writing)</span></li><li><span class="name">#knowledge note that they flipped the RGB for Vgg16 as it uses a different setup (its from back in the day) - but the way they did this was add a layer at the start of the net that just makes these small changes v clever!</span></li></ul></li><li><span class="name">Lecture2 Conv Nets #lecture2</span><ul><li><span class="name">#knowledge for logloss problems, can be very useful and improve score massively to clip your results at approx your validation accuracy - think about how to do this for &gt;2 categories? - as getting it wrong FUCKS you but getting a 0.95 instead of a 1 makes basically no difference</span></li><li><span class="name">#knowledge always good to lower the LR after a bit. aka half or so (maybe a bit less) of planned epochs? and down to *0.1?</span></li><li><span class="name"> #assessment when figuring out whats going on on your model check out:</span><ul><li><span class="name">1. a few correct labels at random</span></li><li><span class="name">2. a few incorrect labels at random </span></li><li><span class="name">3. the most correct labels of each class</span></li><li><span class="name">4. the most incorrect labels of each class</span></li><li><span class="name">5. the most unsure (ie closest to 1/n percent)</span></li><li><span class="name">#assessment and how do we turn this into something to do? well 1, check that these are all reasonable. and if there are reasons they are wrong think about ways to get around it ie are all the wrong ones small?  if so, could build model trained on smaller image sizes. Do we need to crop? etc</span></li><li><span class="name">note that these probabilities aren't THAT much like probabilities in a statistical sense. eg the most uncertain ones will still be v close to 1 or 0</span></li></ul></li><li><span class="name">#jupytertrick %command runs bash commands and ??function gives source code</span></li></ul></li><li><span class="name">Lecture2 aside reading: Stanford CNN course #lecture2 #reading</span><ul><li><span class="name">intro</span><ul><li><span class="name">discussion of techniques like k-nearest neighbour, limitations - for eg shifting, darkening and removing pixels have same distance! (it's literally calculating L1 or L2(euclidean) distance of pixels and voting on label)</span></li><li><span class="name">discussion of k-fold validation, test and validation etc. </span></li><li><span class="name">discussion of the fact that bias can be incorporated into weights (ie add an extra column and an extra value to the images with value 1)</span></li></ul></li><li><span class="name">classification ie loss functions</span><ul><li><span class="name">SVM (support vector machine) where loss is 0 if all 'wrong' class scores are less than a margin under the correct class score, plus a regularisation error which tries to minimise weights (to pick a 'correct' one) </span></li><li><span class="name">Softmax intuits scores as log probabilities, then calculates accuracy that way (discussion about information theory cross-entropy aka where keras name is from)</span></li></ul></li><li><span class="name">optimization aka gradient descent</span><ul><li><span class="name">learning rate is size of step when gradient descending - dont 'overstep'!</span></li><li><span class="name">0 cap on the errors of SVM makes the loss function piecewise linear and convex - NOT true for SGD in neural nets</span></li><li><span class="name">#knowledge powers of 2 vector calcs are quicker hence batch sizes 32/64/128 etc. batch sizes as we 'average' the steps on a small part of the data.</span></li><li><span class="name">brief discussion of approx differential computing (slow) and analytic (fast but error prone - validated with gradient checking ie uses approx to check)</span></li></ul></li><li><span class="name">backprop</span><ul><li><span class="name">backprop uses recursive application of chain rule to calculate gradients for the loss function. typically we do this for weights and biases (training) but also could do it on the input vectors themselves - which is useful (see later) for visualising what the neural net is doing</span></li><li><span class="name">propagate forward through the function as a composition of simple functions (almost like logic gates) then propagate backwards to calculate the gradients, which use the INPUT to the gate and the symbolic differential, timesed by the gradient coming back in (aka the chain rule)! See great diagrams halfway down page http://cs231n.github.io/optimization-2/</span></li><li><span class="name">note that the multiply effects of backprop (swapped input values * gradient for each gradient) imply that if the scale of the input and the weights are very different then gradients change dramatically - hence learning rate must be lowered! So multiplying all input pixel values by 1000 has a massive effect - need to normalize and preprocess!</span></li><li><span class="name">This also works for matrix multiplication operations - need to transpose things properly to dot things to the write shape. Realise that W and dW have same shape so since one of the matrices in the chain rule needs to be transposed, use dimension analysis. </span><ul><li><span class="name">eg if D = W.X where W is shape (5,10) and X is shape (10,3) then D is shape (5,3) as middle dimension where they match is disappeared</span></li><li><span class="name">then dW = something like dD and  X dotted - but we need it to be (5,10) so obviously it has to be dD . X^t (where that means transpose) </span></li></ul></li></ul></li><li><span class="name">setting up architecture</span><ul><li><span class="name">comparison with real brains, pictures of neurons etc where a coarse view shows exactly the rectified linear activation style of neural nets. even 1 neuron can do linear regression, SVM etc (aka combination of linear maps). However real neurons weights arent necessarily linear, so way more complex, also we assume its on the RATE of fire of neurons that makes a difference (aka gives our analogue output) but really timing also matters</span></li><li><span class="name">discussion of different activations</span><ul><li><span class="name">sigmoid</span><ul><li><span class="name">maps well to frequency of neuron view. bad however as near 1 or 0 the gradients are very small aka kills backprop, so v hard to learn. also initial setup can easily 'saturate' the network so it wont learn</span></li><li><span class="name">theyre not zero centered which causes issues - if all data coming into neuron is +, then weights will become all - or + during back prop, and so zig zag. however once added up over a batch we can have mixed sign so thats okay </span></li></ul></li><li><span class="name">tanh </span><ul><li><span class="name">has same frequency stuff as sigmoid, saturation etc, but avoids the zero centering issue so always preferable to sigmoid</span></li></ul></li><li><span class="name">rectified linear (ie max(0,x)) [USE THIS AND MONITOR DEAD NEURONS]</span><ul><li><span class="name">POSITIVES</span><ul><li><span class="name">none saturating nature means that SGD is way faster</span></li><li><span class="name">needs no expensive exponentiation etc doing backprop - just 0s one of the inputs</span></li></ul></li><li><span class="name">NEGATIVE</span><ul><li><span class="name">neurons can die during training. a large gradient flowing through a ReLU neuron can cause weights to update in such a way that the neuron will never activate on any datapoint again. High LR can make up to 40% of the neural net to die. So need to be careful with LR</span></li></ul></li><li><span class="name">tried to fix the neg with Leaky ReLU which have the negative as alpha.x where alpha is v small. Not always consistent results</span></li><li><span class="name">Maxout neuron uses a function that isnt f(wx + b). Namely max(w1x+b1, w2x+b2). Note that ReLU and leaky are both special cases (where w1,b1=0 for ReLU for eg). Therefore maxout has all the benefits of ReLU without dying - but double the params.</span></li></ul></li><li><span class="name">how big a neural net? bigger neural nets more complex, allow more complex classification - more prone to overfitting. however, much better to allow the biggest you can compute, then use other methods (regularisation apparently) to stop overfitting</span></li></ul></li></ul></li></ul></li><li><span class="name">Lecture3 Overfitting and underfitting #lecture3</span><ul><li><span class="name done">#todo see if this lecture talks about training more of the conv net</span></li><li><span class="name">#tool mentions the tool deep visualisation toolbox which seems to allow you to check out the filters of a conv net and what they're reacting to - unsure if its just specific nets or user inputted ones</span></li><li><span class="name">#aside note on dealing with high resolution images. as yet unsolved - typically kaggle winners make images smaller. perhaps should do as eye does - foveation, where the centre is much higher resolution. Ideas like "attentional models" which differentiate between peripheral and focused images</span></li><li><span class="name">#architecture general notes on architecture: 3x3 filters are almost always good enough. main thing is how deep aka how complicated are your issues</span></li><li><span class="name">#finetuning Finetuning:</span><ul><li><span class="name">Depending on what you're doing, could be advantages to essentially run a linear model on the output of the last (aka, adding another dense layer to the model) IF the categories already fit well - eg, dogs and cats (and bones etc that might be correllated)</span></li><li><span class="name">If however it DOESNT fit, but the general filters should (aka theyre all images taken with cameras and not, for eg, drawings, POP the last layer off and add your own dense. Set all the others to NOT TRAINABLE then go for it. This keeps all the filters they've built up and trains just for yours</span></li><li><span class="name">If you want to go further, you could try and train more of the model by now setting it trainable. Typically its good to do the dense layers and leave the convolutional (as theyre unlikely to learn better filters) but it wont hurt to do them too, just would take long and prob not be that useful. Check out the filters if possible and make a decision, or just experiment.</span><ul><li><span class="name">#finetuning #hazard NOTE its ESSENTIAL you train the last layer with everything else frozen FIRST - as otherwise the last layer is random weights and it means the newly unfrozen weights will change loads to try to compensate, throwing away the hard work of the previously trained net!</span></li><li><span class="name">#finetuning #hazard also LR should be very low for this second stage of fine tuning 0.00001</span></li></ul></li><li><span class="name">For the Statefarm problem, its likely that the spatial data is not gonna be different (aka similar filters to see if someones looking away, or at their phone, etc)</span></li></ul></li><li><span class="name">#underfitting #overfitting Underfitting/Overfitting:</span><ul><li><span class="name">overfitting: when training set has much higher accuracy than validation set (you know what it is) vs underfitting: when training set has lower accuracy than validation set</span></li><li><span class="name">Underfitting #underfitting:</span><ul><li><span class="name">can see this with cats n dogs finetuning from VGG. Why? Because of dropout - where p(=0.5) prob of the time any activation is set to 0 and killed (or dropped out) - only when training. This means that half the data is thrown away and is very effective at not overfitting. </span><ul><li><span class="name">if we're underfitting on a VGG finetuned model, we can remove the dropout and see what happens. </span></li><li><span class="name">Easy to remove by just finding the last convolutional layer (as all the dropouts are at the dense layers) and then using predict the train and the val features on that and save it</span></li><li><span class="name">Then recreate the second half of the model with the same architecture, but with dropout with p=0</span></li><li><span class="name">Then when we apply the weights back we need to HALF THEM ALL since during the finetune training process we'll be using all of them instead of just half #todo - think about this and why it should matter - presumably because of backprop)</span></li></ul></li></ul></li><li><span class="name">Overfitting #overfitting:</span><ul><li><span class="name">STEPS FOR OVERFITTING #technique:</span><ul><li><span class="name">1. More data</span></li><li><span class="name">2. Data augmentation</span></li><li><span class="name">3. Use architectures that generalize well</span></li><li><span class="name">4. Add regularization (</span><ul><li><span class="name">note we can't use regularization on a of the data sample, since adding data 'regularizes' already so amount of regularization depends</span></li></ul></li><li><span class="name">5. Reduce architecture complexity (aka remove filters - v hard especially when finetuning)</span></li></ul></li><li><span class="name">#overfitting #technique #regularization Dropout - whilst training, set 0.5 (or some p) of the activations to </span><ul><li><span class="name">Not that bad as long as the validation error doesn't increase also / the difference is huge</span></li><li><span class="name">Dropout better later in the net - aka also means its not a good idea to dropout from the image itself (aka remove pixels)</span></li><li><span class="name">A little bit like random forests! Kind of creating large ensembles</span></li></ul></li><li><span class="name">#overfitting #technique #regularization Adding to the loss function so that higher weights are penalised</span><ul><li><span class="name">L1 regularization and L2 regularization (normal vs squared) </span></li><li><span class="name">Keras supports this</span></li><li><span class="name">Not really used as much as dropout is</span></li></ul></li><li><span class="name">#overfitting #technique Data augmentation</span><ul><li><span class="name">very easy using keras image.ImageDataGenerator with parameters such as rotation etc etc</span></li><li><span class="name">How much to use? Do it and experiment, or do it and look at the images and see if they look like pictures of cats still - aka would never do horizontal flip as cats never upside down</span></li><li><span class="name">#speed NOTE that in CNNs, the convolution is where lots of TIME is taken up (running convolution over whole image) and the dense layers is where the MEMORY is taken up - aside that means that training dense layers is pretty fast. This means for eg if you can pre calculate the convolution layers then that is a great idea. This is more difficult if youre automatically augmenting - maybe faster if you also save the images?</span><ul><li><span class="name">advantage is that the image generator makes unlimited (infinite) random augmented versions</span></li></ul></li><li><span class="name">Obviously don't do this with the validation data!</span></li></ul></li></ul></li></ul></li><li><span class="name">Batch normalization #normalization #overfitting </span><ul><li><span class="name">input normalization </span><ul><li><span class="name">Need the pixel values to be centered about zero and roughly same order of mag as each other as, as we saw earlier in Stanford stuff, the weights will become v difficult to train</span><ul><li><span class="name">this is an issue not just for neural nets but basically for any machine learning problem</span></li></ul></li><li><span class="name">so we normalize the inputs by subtracting my mean (and we can also use standard deviation to make things similar scale, but pixel color values is ok (only between 0 and 255). We got the mean from the imagenet mean aka what the net was trained on</span></li></ul></li><li><span class="name">We need to avoid any weight getting large as then the whole model from that point onwards is fragile (as we saw before). We can't just normalize the activation layers as SGD will just keep increasing it as it think it can get a lot better!</span></li><li><span class="name">We use batch normalization to avoid this and get perks!:</span><ul><li><span class="name">its 10x faster than not, because we can use much higher LR</span></li><li><span class="name">it avoids the influence of one batch on the weights, avoiding overfitting - sometimes. but then sometimes gives the model so much ability to find that it means that you get more</span></li></ul></li><li><span class="name">It works via:</span><ul><li><span class="name">doing the normalization we said wouldn't work but then</span></li><li><span class="name">timesing all activations by a weight and adding a bias (aka introducing arbitrary std deviation and mean) and INCLUDING this in the backprop gradient calculations</span></li></ul></li><li><span class="name">So the model knows it can rescale the weights to make things fit again properly without having to send one weight into overdrive</span></li><li><span class="name">#todo understand why this is a layer in itself aka look at what it actually is</span></li><li><span class="name done">Need to add axis=1 for convolution #todo find out why</span></li><li><span class="name">Unfortunately VGG didnt use BatchNormalization. We cant just add them in after each layer (typically after the dropout) without working out the effect we need to change the already weights by (think about the effect of removing Dropout for eg)</span></li><li><span class="name">the way we do this is simply by initialising the batchnorm with the MEAN and STD VAR of the outputs of the given layer on the original data</span><ul><li><span class="name">eg for VGG, we would have to compute the layer outputs for all of image net for each layer we want to add batchnorm to, and calculate the mean and variance and initialise the batchnorm this way</span></li></ul></li><li><span class="name">Or can just retrain the whole architecture with random weights on imagenet again (obviously need some serious setup to do this - he did it for us)</span></li><li><span class="name">can check this works by repredicting some images and checking the output is the same</span></li><li><span class="name">could then finetune further to see if it can increase!</span></li></ul></li><li><span class="name">Quick aside on LR:</span><ul><li><span class="name">typically start on the default LR, then pump up to max aka 0.1, then lower</span></li></ul></li><li><span class="name">Ensembling #ensembling</span><ul><li><span class="name">Literally just put all of your training steps into a function and run it 6 times or whatever.</span></li><li><span class="name">Then get all the predictions of all of the models</span></li><li><span class="name">Then average them </span></li></ul></li></ul></li><li><span class="name">Lesson 4 CNN review, statefarm biopsy, semi-supervised, collab filters #lesson4 </span><ul><li><span class="name">Optimizers:</span><ul><li><span class="name">Non-dynamic LR</span><ul><li><span class="name">SGD </span><ul><li><span class="name">(note that the 'stochastic' element is that we only use a minibatch of training examples at a time rather than all at once). </span></li><li><span class="name">You know what LR is. Changing it in order not to overstep is ad-hoc </span></li></ul></li><li><span class="name">Momentum</span><ul><li><span class="name">motivation: loss function space consists of saddlepoints, so we get valleys. with normal LR, we zig zag down the valley. Momentum also adds on the average gradient of last few steps to speed this up</span></li></ul></li></ul></li><li><span class="name">Dynamic LR</span><ul><li><span class="name">Adagrad</span><ul><li><span class="name">motivation: we see that (even when just training y=mx+c sometimes, say m is 2 and c is 30) sometimes one variable gets to the minimum quickly and the other takes ages. Can't just crank the LR as this would throw the smaller variable out</span></li><li><span class="name">Adagrad avoids this by creating a different LR for each parameter</span></li><li><span class="name">Divides the LR for each param by the L2-norm of the previous gradients =&gt; if gradients have been large, it slows them down. if gradients have been smaller, it speeds them up</span></li><li><span class="name">difficulties: </span><ul><li><span class="name">l2-norm is strictly increasing, so it may slow down LRs before we reach minima (and make params effectively fixed)</span></li><li><span class="name">can be computationally expensive to calculate all the L2-norms</span></li></ul></li></ul></li><li><span class="name">RMSProp</span><ul><li><span class="name">motivation: avoid difficulties of adagrad</span></li><li><span class="name">weights the more recent gradients when calculating L2-norm so can increase the LR. has the effect of pushing things out of local minima, and slowing things down when they speed towards big minima</span></li></ul></li><li><span class="name">Adam (this one is a ting. Perhaps RMSProp is better when fine-finetuning cause momentum can be a bit much?)</span><ul><li><span class="name">RMSProp + momentum</span></li></ul></li><li><span class="name">Eve</span><ul><li><span class="name">Adam, and increases LR when loss stabilises</span><ul><li><span class="name">difficulty: we expect loss to stabilise at global minima! so eve will pop us out. I think its basically shit</span></li></ul></li></ul></li><li><span class="name">#todo check out the other optimizers keras has native support for (aka Nesterov momentum)</span></li><li><span class="name">#todo check out jeremys idea of doing Eve style, but looking at gradients instead of loss</span></li></ul></li><li><span class="name">In general, the problem of automatically changing the LR has been difficult to solve since the loss function can zig zag like crazy. Best thing is probably to save weights as you go (or every 5,10 depending on how many epochs). Note that you dont necessarily want the LOWEST validation loss - amidst the noise, this is overfitting!</span></li></ul></li><li><span class="name">Statefarm biopsy #assessment #statefarm:</span><ul><li><span class="name">what can we gain from using a much quicker sample? </span><ul><li><span class="name">firstly, what does it take to create a better-than-random model? try a single hidden layer and check it out</span><ul><li><span class="name">#technique note instead of worrying about normalizing input w/ mean and std, make the first layer a batchnorm - it'll sort it for you</span></li></ul></li><li><span class="name">Nothing happened - huge loss etc. It had 1.5mill params, so it cant underfit! So must be jumping too far</span><ul><li><span class="name">#assessment note that at the start its REALLY easy to jump too far! theres always 'reasonably good' answers that are way too easy to find - aka, always predict 0 (or 1/n or whatever), but note this is impossible w/ softmax - so its likely that its gonna predict 1 class all the time </span></li></ul></li><li><span class="name">So sometimes best to start v low LR once away from that danger zone, then crank it up again</span></li><li><span class="name">#technique good to try each type of data aug by itself to figure out what works well (on a sample, 3 or 4 different values) combined with thinking about the images and what would work. Then combine them together to make your data aug setup</span></li></ul></li><li><span class="name">#finetuning NOTE that if you plan on just training the dense layers from scratch (aka not finetuning them really) you obviously dont have to train the last layer first. Also in this case, you don't need to shuffle the data (perhaps because there's enough complexity in the dense layers to handle it)</span></li><li><span class="name">pseudo-labelling and knowledge distillation #pseudolabelling #knowledgedistillation</span><ul><li><span class="name">in state farm, the test set is much larger than training set. it'd be great if we could use some of the images to train in some way</span></li><li><span class="name">can we use the unlabelled data to tell us something about the structure?</span><ul><li><span class="name">1. use model to predict labels on val (or test) set</span></li><li><span class="name">2. combine the labels and the features/inputs</span></li><li><span class="name">3. train a new model with same architecture (and same validation set that now you're pseudo training on)</span></li><li><span class="name">1/4 to 1/3 of your minibatch should be pseudo labelled</span></li><li><span class="name">in my mind, obviously introduces overfitting wrt the labels that are labelled corrrectly - but then thinking about how humans work if you see something you reckon is a 7, you update your knowledge about 7s to try to incorporate if possible (aka, pseudo labelling?) #todo read paper http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf</span></li></ul></li></ul></li><li><span class="name">#collaborativefiltering and #embedding</span><ul><li><span class="name">collab filtering is so good that metadata doesnt even help - the idea is that instead of ASKING people what they like, we'll figure it out</span></li><li><span class="name">idea is for every user (or training user / high volume user) and every movie (likewise) we have a number of important qualities, like percentage action, if bruce willis is in it etc - for the users this is how much they like these qualities. then the rating is just the dot product</span></li><li><span class="name">how do we know these features? we don't. we learn them, randomizing first and using gradient descent (measuring our rating vs true user rating) these are know as <span class="contentItalic">latent factors</span></span><ul><li><span class="name">we can then afterwards graph which movies are high in one factor and low in others etc to figure out what each factor means</span></li></ul></li><li><span class="name">then the BIAS is how good the movie is, and how much the user likes movies!</span></li><li><span class="name">#embedding comes from how we map the user id to their set of latent factors</span><ul><li><span class="name">we could one-hot encode and then matrix multiply with the matrix whose columns are each users latent factors, but this takes a long time for many users and is redundant as our first matrix is the identity - we just want each column</span></li><li><span class="name">keras does this for you in the embedding layer thats what it is</span></li></ul></li></ul></li></ul></li></ul></li><li><span class="name">Lesson 5 Intro to NLP and RNNs #lesson5</span><ul><li><span class="name">how to upd</span></li><li><span class="name">Overview of embedding and the movie rating collaborative filter from end of lesson 4</span></li><li><span class="name"> Keras functional API (vs the Sequential API)</span><ul><li><span class="name">sequential is a subset of functional - functional allows you to input things at any stage ie you can have a model that takes two inputs, or equally for CNNs you could add metadata which simply links straight into the Dense classifiers at the end of the model</span></li></ul></li><li><span class="name">Analysing collaborative filters #analysis</span><ul><li><span class="name">a good thing to do is to use principal component analysis #pca #principalcomponentanalysis</span><ul><li><span class="name">version in sklearn</span></li><li><span class="name">basically just chooses a direction with maximum variance, then chooses the next highest variance in a direction orthogonal and so on (up to a user defined num of dimensions)</span></li><li><span class="name">has the effect of distilling the knowledge, identifying components that move in similar directions</span></li><li><span class="name">in the movie case, it combines latent factors into 3 major categories that end up quantifying different qualities of movies - eg how 'classic' they are, how violent they are</span></li></ul></li></ul></li><li><span class="name">Into to NLP #naturallanguage #nlp</span><ul><li><span class="name">setup</span><ul><li><span class="name">first using the IMDB data that comes with keras #imdb #sentimentanalysis</span></li><li><span class="name">has loads of IMDB reviews then labels that say if they're saying its good or bad</span></li><li><span class="name">works by mapping each word to a index, then each review is an array of numbers</span><ul><li><span class="name">the index is of a big array of words, ordered by how frequently they're used</span></li></ul></li><li><span class="name">we cap our vocabulary by taking the first ~5000 elements of the word list and then map every higher (rare) word to just the last index. Then this word simply becomes a stand in for (rare word) which we assume won't have too much effect on the sentiment</span></li><li><span class="name">we also cap our review length (here ~500) and pad the shorter ones with 0s at the start (and cut the longer ones)</span></li><li><span class="name">then we embed the word indices. we use 32 #latentfactors since examples with 50 have dealt with vocabularies of size 50k, so 32 should be enough!</span></li></ul></li><li><span class="name">attempt 1 - then the standard flatten, relu layer, sigmoid layer for classifier (since we're predicting 0 or 1 - equally we could one hot this and use softmax). 88.0% accuracy so slightly under academic state of art at time of writing (88.3%)</span></li><li><span class="name">attempt 2 - we know that locality is going to be a big factor! so why not use convolutions? we get 89.5% accuracy  vs 88.3% academic state of art</span><ul><li><span class="name"> in this case, we'll be using 1D convolutions since we want all the latent factors of each word. effectively the latent factors will act just as the channels in the image did before</span></li><li><span class="name">first guess is 5 words at a time in a filter, and 64 filters in a layer, and border_mode=same</span></li><li><span class="name">note we also dropout before and after conv, and maxpool1d after,</span></li></ul></li><li><span class="name">pretraining #finetuning #pretrained </span><ul><li><span class="name">analogously to pretrained image nets, we can share things about the words</span></li><li><span class="name">specifically, we dont need a whole load of filters to say what is or is not a dog - we only need to share the embedding! the rest of the model is likely to be very specific and the 'meaning' of the word dog (or any word) is supposed to be captured by the embedding, with the rest of the net working WITH that for a specific aim</span></li><li><span class="name">in other words, we dont need to share large model weights. we need to share word embeddings or as they're known, WORD VECTORS! #wordvectors #wordembeddings</span></li><li><span class="name">available word embeddings: </span><ul><li><span class="name">word2vec: google, really good documentation from tensorflow </span></li><li><span class="name">glove: out of stanford and are supposed to be good, good tokenisation </span></li><li><span class="name">#todo: research (better) word embeddings at time of writing?</span></li></ul></li><li><span class="name">again, check the corpus they've been trained on, if theyre cased and uncased etc, tokenisation aka what theyve decided is or is not a word</span></li><li><span class="name">aside - how are they trained? very clever </span><ul><li><span class="name">there's no labels in these huge corpuses so they need to to unsupervised learning</span></li><li><span class="name">they get strings of 11 words, copy it and replace the middle word with a random word - aka v v v v v likely to be semantic nonsense</span></li><li><span class="name">then the real sentence gets label 1, fake sentence gets label 0 </span><ul><li><span class="name">essentially labelling whether the sentence makes sense</span></li></ul></li><li><span class="name">then they train the embedding (and obviously classifier)</span></li><li><span class="name">note that word2vec and glove used linear models to train because it was easier and they wanted to do linear stuff. note the none linear stuff is only AFTER the embedding (aka the classifier) so its fine with regards to the embeddings. I guess a deep model could be better but could be v v v long</span></li></ul></li><li><span class="name">also we see that embeddings are very similar across languages! so we can translate by finding the nearest word in the vector space in the other language</span></li></ul></li><li><span class="name">always used #pretrained #embedding s !! itll be way, way better. need to load them off disk and make an embedding matrix that works (aka take the embeddings and put them in the right order for the indices). if it doesn't exist in the pretrain, randomly create it</span></li><li><span class="name">analysis</span><ul><li><span class="name">we can use TSNE (t distributed stochastic neighbour embedding) to lower the dimensionality of the vector space to have a look (a ~little bit~ like PCA but not really) #tsne</span></li></ul></li><li><span class="name">quick thing about using power of functional: implementing Inception blocks, by just doing each convolution size with the functional input and adding them to an array, then using merge(mode='concat')(array) to stick them together at the end</span><ul><li><span class="name">note that if you simply save the input and output of this in variables, then do </span></li><li><span class="name">block=Model(input,output)</span></li><li><span class="name">you can use it in sequential mode! as 'block'</span></li></ul></li></ul></li><li><span class="name">Quick into to RNNs #rnn</span><ul><li><span class="name">imagine a model trying to understand html</span><ul><li><span class="name">needs to understand the relevant text that we want to read out</span></li><li><span class="name">but also needs to keep track of the tags </span></li><li><span class="name">aka have a 'memory'</span></li><li><span class="name">so then it can handle long term dependencies (aka the tag being open)</span></li></ul></li><li><span class="name">what about handling the two sentences 'I went to Nepal in 2009' and 'In 2009, I went to Nepal'</span><ul><li><span class="name">it needs to understand these as the same sentence</span></li><li><span class="name">needs to understand the in 2009 bit, and keep that as a STATE </span></li><li><span class="name">then its gonna have to remember that state till the end of the sentence</span></li></ul></li><li><span class="name">Further, we need to be able to handle things of various lengths.</span></li><li><span class="name">These are things CNNs can't do, and what RNNs can. AKA the four things:</span><ul><li><span class="name">Variable length sequence</span></li><li><span class="name">Stateful representation</span></li><li><span class="name">Memory</span></li><li><span class="name">Long term dependenc</span></li></ul></li><li><span class="name">Use examples:</span><ul><li><span class="name">Swiftkey, identifying house numbers from google streetview (uses conv net then RNN to do attentional stuff) or generating random math papers via latex (aka remembers tags and stuff)</span></li></ul></li><li><span class="name">So a basic idea of capturing state, say we're trying to guess the 3rd word in a sequence. we could do word 1 input then FC layer then second FC layer but give word 2 as an input into the second FC layer. this obviously could be expanded to more words. Then the matrices are capturing state! as we wished. as its getting an input but has already got information about the context </span></li><li><span class="name">so that begs the question... what if we just feed the fc layer back into itself, to predict word 3 from our prediction of word 2? and so on. hence RECURRENT NN</span></li></ul></li></ul></li></ul></li></ul></body></html>